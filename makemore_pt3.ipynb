{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(len(words) * 0.8)\n",
    "n2 = int(len(words) * 0.9)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])            # 80% of data\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])        # 10% of data\n",
    "Xte, Yte = build_dataset(words[n2:])            # 10% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),               generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden),   generator=g) * (5/3) / ((n_embd * block_size)**0.5) # 0.2\n",
    "b1 = torch.randn((n_hidden,),                       generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),            generator=g) * 0.01\n",
    "b2 = torch.randn((vocab_size,),                     generator=g) * 0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3147\n",
      "  10000/ 200000: 2.1984\n",
      "  20000/ 200000: 2.3375\n",
      "  30000/ 200000: 2.4359\n",
      "  40000/ 200000: 2.0119\n",
      "  50000/ 200000: 2.2595\n",
      "  60000/ 200000: 2.4775\n",
      "  70000/ 200000: 2.1020\n",
      "  80000/ 200000: 2.2788\n",
      "  90000/ 200000: 2.1862\n",
      " 100000/ 200000: 1.9474\n",
      " 110000/ 200000: 2.3010\n",
      " 120000/ 200000: 1.9837\n",
      " 130000/ 200000: 2.4523\n",
      " 140000/ 200000: 2.3839\n",
      " 150000/ 200000: 2.1987\n",
      " 160000/ 200000: 1.9733\n",
      " 170000/ 200000: 1.8668\n",
      " 180000/ 200000: 1.9973\n",
      " 190000/ 200000: 1.8347\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Batur\\Desktop\\python\\makemore\\makemore_pt3.ipynb Cell 7\u001b[0m line \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Batur/Desktop/python/makemore/makemore_pt3.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m bnstdi \u001b[39m=\u001b[39m hpreact\u001b[39m.\u001b[39mstd(\u001b[39m0\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Batur/Desktop/python/makemore/makemore_pt3.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m hpreact \u001b[39m=\u001b[39m bngain \u001b[39m*\u001b[39m (hpreact \u001b[39m-\u001b[39m bnmeani) \u001b[39m/\u001b[39m bnstdi \u001b[39m+\u001b[39m bnbias\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Batur/Desktop/python/makemore/makemore_pt3.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Batur/Desktop/python/makemore/makemore_pt3.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     bnmean_running \u001b[39m=\u001b[39m \u001b[39m0.999\u001b[39m \u001b[39m*\u001b[39m bnmean_running \u001b[39m+\u001b[39m \u001b[39m0.001\u001b[39m \u001b[39m*\u001b[39m bnmeani\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Batur/Desktop/python/makemore/makemore_pt3.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     bnstd_running \u001b[39m=\u001b[39m \u001b[39m0.999\u001b[39m \u001b[39m*\u001b[39m bnstd_running \u001b[39m+\u001b[39m \u001b[39m0.001\u001b[39m \u001b[39m*\u001b[39m bnstdi\n",
      "File \u001b[1;32mc:\\Users\\Batur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:130\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m    128\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "    # minibatch construction\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # linear layer\n",
    "    hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
    "    #BatchNorm layer\n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer activation\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # cross-entropy loss\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b923aeea10>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA0lklEQVR4nO3dd5hU1fnA8e/LAotSpC0dXEAs2EBWEBVsiCgJWCNojBp7NGr8JRElwQRjYjRRY8SCLdFoEGtIQFEUCyjIIkWKC0sTkLLSO+zu+/tj7ix3Z6fc6bMz7+d5eJi5c8vZKe+995z3nCOqijHGmNxRJ90FMMYYk1oW+I0xJsdY4DfGmBxjgd8YY3KMBX5jjMkxddNdgEAtW7bUwsLCdBfDGGNqldmzZ3+vqgVe1s24wF9YWEhxcXG6i2GMMbWKiKzyuq5V9RhjTI6xwG+MMTnGAr8xxuQYC/zGGJNjLPAbY0yOscBvjDE5xgK/McbkmKwK/JMXrmfjjr3pLoYxxmS0rAn8+8oruOnl2Vzx7Mx0F8UYYzJa1gR+/3wy327end6CGGNMhsuawG+MMcabrAn8+w5UArC/vDLNJTHGmMyWNYF/94HydBfBGGNqhawJ/MYYY7zJmsBfRyTdRTDGmFohawJ/YNift3or3+/cl5ayGGNMJsuawB8Y+YeOmc6Qv09LT1mMMSaDZU/gD+K7bdaL15hsUFmpnPqnD3lnztp0FyUrZE3glxqVPcbUtHNfOT954UvWbt2T7qKYKOwrr+S7bXsZ8db8dBclK3gK/CIySERKRKRUREYEef0aESkTkbnOv+tdr10tIkudf1cnsvDVyxDbdqrKS1+sZPf+xKeDLivbyc0vz2ZfeUVc+ymvqKRwxERenL4iQSXLXGu2JLfn9aT56/h0SRmPfbAkqccxJpNFDPwikgeMAc4HugPDRaR7kFVfU9Uezr/nnG2bA/cBfYDewH0i0ixhpXeXM8r1S9bvYO+BCqYs3sio/yzkT5O+SXiZfvP2At5buJ7ZK7fEtZ/dB3wnjkfez+5g9XHJRk7/81Te/XpduotiTFbzcsXfGyhV1eWquh8YBwz1uP/zgA9UdbOqbgE+AAbFVtTwJIpL/p37yjnvsU+5c9zcqiv9rXsOJKNYJgoLv9sOwNdrt6W5JCZT+cfkSpb12/by/sL1yT1IBvAS+NsDq13P1zjLAl0iIvNF5A0R6RjNtiJyo4gUi0hxWVmZx6IH7CPE8sIRE5kUcAW517mCnrVyc0zHygTvLVjvuZ560XfbKd24I8klykylG3ew8Ds7kdR2qeqmc/GT07nx5dmpOVgaJapx979AoaqegO+q/p/RbKyqY1W1SFWLCgoKYiqA+4txoKL6eD1jppZGvb9F321n6+79MZUlFW7+12yGPjHd07oXPP4ZAx75NOFl2LRzHzOXb0r4fhNpwCOfMvjxg2m9SpIvGU2tliuZgF4C/1qgo+t5B2dZFVXdpKr+3lLPAb28bpsMa7fEn7FxweOfceEYb4E1XdLdQe2yp7/g8rEz0lqGWHm9ghz930X0e+ij5BbGmBTzEvhnAd1EpLOI1AeGARPcK4hIW9fTIcBi5/FkYKCINHMadQc6yxLOnc7594+iv8IPZuWmzBjbP9n1mrFa/v2udBch6V6YvoLVmy31M1Nk6E+h1okY+FW1HLgNX8BeDIxX1YUiMlpEhjir3S4iC0VkHnA7cI2z7Wbgfnwnj1nAaGdZ4rmu4N78ak3A3xB5c83U6OpmXRViVlmpbNtdvQF/8brt3PPWfCora8FnnyFWb96d3t+KfVQJ4amOX1UnqeqRqtpVVR9wlo1S1QnO43tU9VhVPVFVz1LVb1zbvqCqRzj/XkzOnxH+1j1cbng02UCh7CuvoGxH6GoX+656Eyqg7C+v5OQHpsSV5vnE1FJOHP0+G7cf/Jyu/2cx//5yNd9ti3xFv3jddrreO4kFa7cxvnh1xPWz0Tfrt9Pvoak891nq+5PYGIyJlTU9d8PZvteXsjlv9daQE7X8b/465nxbM9++cMREPly8oer56s27mRHQoPmzf33FyQ9MqbFtbfuyVlZqjYbxWFz+zBf86OkvYt4+8H3btGsfZTv28fv/Lop5n+8t8KXobXRO0NH29H5l5ioqKpUf/H0av35jPiuSWM21e385w8fOYFnZzqQdIxbfOlWfM1d4v2kvWb+jartEsMb5xMiawB/pZ7y8bCdDx0zn8rFfcONLxUHXeWXmt0GX/2vGqqrH/R6ayrCxM9i6ez8nPzCF2au28OE3G8MeOxNrkSoqlWc/XV6V2tp91Ht0uXcS3Ua+G/e+Z67YzJcZmir7suuzjEd5Ak6QoUwv3cQXyzfxp0mLI6+c4c577FP6Pzw17v3YkCyJlTWBP5ItTmrmnG+38tW3WwHflWWs9ZUzlm+mbMc+Lnnq8xqvrfx+F99u2p3RV/wT5q3lgUmLedQZumD3/oPDStz79tcAbNy+N2GBMtXKKyqt7j4Ks1ZuZvXmzEhmCGbHXutgmUhZE/gj1dVf8lTwqgd/b9Hqy6p3+Pk2yh/EmX/5mP4PT2XnvoPBdMqiDXwVUJW0Y++BoB3M/Ea+/TWFIyYmpZHAH+j91WBurzp3Pje8PJvfvrOA/83/jg8Xb6B0Y3KrHt78KnimbzTn5r0HKti5r5wjRr7L8GdDp5pm8kk5HS57+gv6PRT/lXmyXPZM7FWHpqbsCfwJ2s/E+euqdfgBWFYWW33uvNVbAahQ5fqXirn4yep3Byu/951Qnvy4lK2799e4Qg2sekp1rNqyy3eXdNurc7jun8UMeOSToOu9PGMVhSMmsnNffAPdRao39xKsz/nrJxx3ny9jOFxddLyBP3D7QY99ythPl8W3UxPScuc3mInVprVR1gT+WHy/cz9LNlQfymCpx6ENohlFsjLCt3Xr7gP0GP0BD00uCfr6Lmc8oXgD68OTv2Hj9uo9E9d7yGiJ5LfvLAAIm9nkdsu/ZvMP10ijByoqmbb0+7jKMGHedxSOmBhyGItkx4tv1u/gj0kY6C9ZVJW/TVnKqk3Ja6T+6JsNkVcyaZE1gT/WK7iPSw6ODRTN1cQfJnpvePtLiIDut9XJL39vQegqH4BKhV++Po/Zq2peyd7/v0WM+s+CsD/kMVOX8YOAWcmmlsQ2NlIoXrJ53l2wnt+5MnT+MrmEHz8/s+p5LA15bwf03Ui0mt+N2l1XtG7bXh6dsoRrX5yVtGP89B/BkyhMcHO+3cKG7akZMiJrAn+ivBoisycewdoRorHPlYL6xuw1XPX8lzXWeX7aCl76YhVnPPxx2H0FS2f8YFHirszc2Twj3gw/aYZ/noJIVWmZeHefijaCnfvK+bgkfMZYrPx3ofsC0pv9WV5+yRwW5OInp/ParOh+b8n4LuzZX8Hz01akPRngoic/55y/Bq9OTbSsCfyJSvfa6LG6Ih7lFZX8bcrSqiqcQHsPVETVWeniJ6MfU+iduQcbUm8Ikd4ar3Gzwnd0uvWVOUGXPzG1lPHFqymvqOT2f8/hm3W+E2c8n7DX/gmvzvyWL6PIU/dif7lvIp2nPwneBrBp5z4KR0xk4nzfZ+7/O2cs38w1L84Kehe390AFi+K8oAjm6N++xwJnWOwxU0sp+sMUJsz7DnCPZpuYAPnVt1u5+82vI67nvjBJRq/hhyeXcP//FvHugvQPxxxvda5XWRP4EyFVXdHfmrOWR6csqco6CQxoD0xczC2vfFX1PNLwy/70VLcnPlpK4YiJnPZgzQHG3l+43lNwW5/AkQorK5VH3i+p1g4wZXHoO41nP13OsrJdTJj3Hdf9M/4TU82MpOCnkXvf/pofJTiDZI+TQfVkiFFil2zwle2lL1YGfd2dauv3qzfmc8Hjn7F518ERZK98bgbX/SP+qht/4H/YqaL8coWvw+KzTo/dAxXhfydzV2/lpPs/qLbs+Wkr6Dn6/ao7ilAdKYMJ9b4kyjZnLo5kzMKXqbIm8Cfi1vs/zpVNPLxMBu2/vfafZ3YEnOUDG45XBfR8rHDdkm4Pkd/8F2e2rmAnjV9HqILx25/ATkpfrtzM4x+VcrfHY4dz1fMzGTpmOv+Zu5bCERPZvvdAwqsAdgV8JqnMsCrzUL3y1SpfarA7WE0v3VTVmfDDxRvC9vwNrNJJpCc+WlrthAS+Nqgtuw9UpUbfN2Fh0o4fq1DfIVVlwrzvqv3uarusCfyJkIgP9s7X5oZ9PXCgsFi462WHBDTWerE1jjL4qyNK1kc3sYu/nSMw4Pxn7lqC/eSWBukz4O+r8dnS75m3eit3jJvr24eHk20we5yyBKveO/MvH3veT6LvFJ/7bHnEdTbu8N2NBWsfKVm/g+v+WRy2vjgZ8zN4MfDRTzlQURl0eJRQ3G9vMmJvpIvGt+es5fZ/z/H0udQWFvgTyEv1yaC/hf7BacD/XgQbOjoRJ5dQbn31K7qPeo/rX6pZpXBZmIye+/8XfJydO8bNDVl14HVclvJKrZad5ZX/qvTiJz9n1H8WVPv8ynbsC3k3BaE7DO4vr2RywNR9kf6OwNcjNXZv2bW/6j27+oWaDf3nPeY9qPtPIG6Bf1qia0CjGQ/qi2WbqpIA/AI7QrrtPVDBP6aviO0iLsQmm3b6vide05VrAwv8Hq3evJvxERorvdQNr9u2l7++Hzy9M7BKJ1bjosyUCKZwxMSQr+3eX8GGbTV/BMEyQHrd/wGPTYltkvjA6pZECgxuL32xqsbn98vx80JvH2L5o1OWcNPLs/l0Sc0TUaTe5V6rK6OZHzrwJBQoUn19OpVu3MnwZ2cwa2X1QP9AmFTqJz4q5Xf/XUTXeyd5viv1v+1e+/A899ly7opwZ5/psibwJzu9rt9DUz3XjUcST1WLF396N3M6Em3atZ/HpiwNu06o0BM4zIbXOYa98PJ1eT+GNNc1zuxvW1I4bed7C9aHHE7jppdnV+u0t2nnPhaviy8bKNJvLdo7hFBVZeHuuEJxD7Fd7OrvsnnXft4K0dfD//c863G46T9MXMxbQaoXl27YwVtfranK0pqyaAO/m7CQlwMapzds35vUNhYv6qb16KaGikqNKuMhXRI5PG6wq+NoxHJXPyvO0UNfK17N3YOO5plPlkU8ISU7Wezmf4WfHNzdSD/ob59FrLIITI1eHqHqae+BCrbs3k/bww6JUNLgLh87g/E39Y16u+VlO1m1eTfPfbacZ39SxKH16/JdiM/iln/NZuaKzZxc2JyOzQ+NuO/vd+6jZaN8Ln3qc4pXeWuPOPdRXxXbv67rA8CLn69geqkvI2pY706M/XQ5153emT5//BCA3ww+huv7dfG070TzdMUvIoNEpERESkVkRJj1LhERFZEi53mhiOwRkbnOv6cTVfBs9bNXZvP5ssyewDweyfjbQrUfhONPoYzVUx/7cvL/9O43vPTFwRFMt+wKfaXvv7JctWlX1fapMLWkjE+XlFFRqUGDfqQ2oS8C5p9QpVq9+yVPfU7fP8U+L3GwtrFzH/mkxthWgc7+6ydc++Isppdu4vPSmt+r8bNWV93d+Bvwg7UvBOsD9M06X7WP16Af6eQ/vng1D08u4QnXtLDR9P5PtIhX/CKSB4wBzgXWALNEZIKqLgpYrzFwBzAzYBfLVLVHYoobppy1vAu93+SFtWN8k3TVDd81fm5ajuvVtNKaYw4FDm535XMzWbNlDz8q6kCLRvkRW/Pjrcb0j6U0sHvroK8/Ny1ytsotrruKT5aUcdRv3mPlg4OBgxlbpz34Ec9fU+SpTJHuaoNldfnNXrUlZBuU++5q3pptnP+3zzyVJ17Bqtq+WrW16rG/L0fJhuiy4ZLFyxV/b6BUVZer6n5gHDA0yHr3A38GUjPYhMlJb4UYujmTbXLdBbw689uqdoBJC9ZXG9XUy8VL2Y59EQf9CyVUm8XfPwroWBakGF56ta7duodBj30WcWKi8kpleZJmMIv0zgR7PdSJNdJAjAvWbquqMnw6yB3cniD1+IkcHiUeXgJ/e8CdzrLGWVZFRE4COqpqsNNwZxGZIyKfiEi/YAcQkRtFpFhEisvKYqvvtfHVTabbuvtA1SQ3cPBK3N+p6Zv128P2CVi9eTcnPzAlaJBJJK8/pVWbdoXN/grF3xckXfwjud45bk7ICV4+/GZDjQENn5u2olq12A/+Pi1sCrMX44tXV00LmkpxN+6KSB3gEeCaIC+vAzqp6iYR6QW8IyLHqmq1tAJVHQuMBSgqKsrc/DJjXKZ4uHqLZpCzLbsP8Prs0KOM+uuRP4mzMTxRwuXTh5OIeZ1DidSZ7qmPl/GG8x6/M/c73pkbvLf+i9NXBl1+4uj3ayy79sUvYx5j59dv+DIF/dVmqeLlin8t0NH1vIOzzK8xcBzwsYisBE4BJohIkaruU9VNAKo6G1gGHJmIggeyC36Tatd7GNzurjB9AfzcE8b4GxXDSfbd7WNTlvL8tMipjb94LfLf5lW8o5DeMc434F+kWrA3wpxYYzW1pIyv124Lu06kPhyp5iXwzwK6iUhnEakPDAMm+F9U1W2q2lJVC1W1EJgBDFHVYhEpcBqHEZEuQDcge/o9GxPCIx8s4Yd/n8Y6D30PpgdpEHa7c9zcatUBG7Yntwfp2q17YsqU8ipYeug1cc4LsGt/BeUVlZ6zcHJdxMCvquXAbcBkYDEwXlUXishoERkSYfP+wHwRmQu8Adysqokd89aRaWdUk9tWbdrN12u3URpmoLRgXphe80r7m/U7Iubq1yb/+Hxl0OWTF65n4KOfsDTGzJcjRr4bR6kS5+EgPfMXRLgjSDVPdfyqOgmYFLBsVIh1z3Q9fhN4M47yGVOreUnACTbscvD1snvY4Jte9p3cRifxbiMV/HNtu70dYSDBZE54E0z2DNmQ7gIYk2Q2lWH28pIokEhZE/iNMdnhs6Xh2zyy0Yi3Is9ElkhZE/itit8YY7zJmsBvjDHGGwv8xhiTY7Im8Fs6pzHGeJM1gd8YY4w3FviNMSbHWOA3xpgcY4HfGGNyjAV+Y4zJMRb4jTEmx1jgN8aYHGOB3xhjcowFfmOMyTGeAr+IDBKREhEpFZERYda7RERURIpcy+5xtisRkfMSUWhjjDGxizgRizN14hjgXGANMEtEJqjqooD1GgN3ADNdy7rjm6rxWKAdMEVEjlRVbzNPGGOMSTgvV/y9gVJVXa6q+4FxwNAg690P/BnY61o2FBjnTLq+Aih19meMMSZNvAT+9sBq1/M1zrIqInIS0FFVJ0a7rbP9jSJSLCLFZWVlngpujDEmNnE37opIHeAR4P9i3YeqjlXVIlUtKigoiLdIxhhjwvAy2fpaoKPreQdnmV9j4DjgY2do5DbABBEZ4mFbY4wxKeblin8W0E1EOotIfXyNtRP8L6rqNlVtqaqFqloIzACGqGqxs94wEckXkc5AN+DLhP8VxhhjPIt4xa+q5SJyGzAZyANeUNWFIjIaKFbVCWG2XSgi44FFQDlwq2X0GGNMenmp6kFVJwGTApaNCrHumQHPHwAeiLF8xhhjEsx67hpjTI6xwG+MMTnGAr8xxuQYC/zGGJNjLPAbY0yOscBvjDE5xgK/McbkGAv8xhiTYyzwG2NMjrHAb4wxOcYCvzHG5BgL/MYYk2Ms8BtjTI6xwG+MMTnGAr8xxuQYT4FfRAaJSImIlIrIiCCv3ywiX4vIXBGZJiLdneWFIrLHWT5XRJ5O9B9gjDEmOhEnYhGRPGAMcC6wBpglIhNUdZFrtVdV9Wln/SH4Jl8f5Ly2TFV7JLTUxhhjYublir83UKqqy1V1PzAOGOpeQVW3u542BDRxRTTGGJNIXgJ/e2C16/kaZ1k1InKriCwDHgJud73UWUTmiMgnItIv2AFE5EYRKRaR4rKysiiKb4wxJloJa9xV1TGq2hW4G/iNs3gd0ElVewJ3Aa+KSJMg245V1SJVLSooKEhUkYwxxgThJfCvBTq6nndwloUyDrgQQFX3qeom5/FsYBlwZEwlNcYYkxBeAv8soJuIdBaR+sAwYIJ7BRHp5no6GFjqLC9wGocRkS5AN2B5IgpujDEmNhGzelS1XERuAyYDecALqrpQREYDxao6AbhNRAYAB4AtwNXO5v2B0SJyAKgEblbVzcn4Q4wxxngTMfADqOokYFLAslGux3eE2O5N4M14CmiMMSaxrOeuMcbkGAv8xhiTYyzwG2NMjrHAb4wxOcYCvzHG5BgL/MYYk2Ms8BtjTI6xwG+MMTnGAr8xxuQYC/zGGJNjLPAbY0yOscBvjDE5xgK/McbkGAv8xhiTYyzwG2NMjrHAb4wxOcZT4BeRQSJSIiKlIjIiyOs3i8jXIjJXRKaJSHfXa/c425WIyHmJLLwxxpjoRQz8zpy5Y4Dzge7AcHdgd7yqqserag/gIeARZ9vu+OboPRYYBDzpn4PXGGNMeni54u8NlKrqclXdD4wDhrpXUNXtrqcNAXUeDwXGqeo+VV0BlDr7SwqRZO3ZGGOyh5c5d9sDq13P1wB9AlcSkVuBu4D6wNmubWcEbNs+yLY3AjcCdOrUyUu5jTHGxChhjbuqOkZVuwJ3A7+JctuxqlqkqkUFBQUxl+GPFx0f87bGGJMrvAT+tUBH1/MOzrJQxgEXxrhtXIb2aJesXRtjTNbwEvhnAd1EpLOI1MfXWDvBvYKIdHM9HQwsdR5PAIaJSL6IdAa6AV/GX2xjjDGxiljHr6rlInIbMBnIA15Q1YUiMhooVtUJwG0iMgA4AGwBrna2XSgi44FFQDlwq6pWJOlvMcYY44GXxl1UdRIwKWDZKNfjO8Js+wDwQKwFNMYYk1jWc9cYY3JMVgV+1cjrGGNMrsuqwG+MMSYyC/zGGJNjLPAbY0yOydrAf9gh9dJdBGOMyUhZG/gHHNM63UUwxpiMlFWB35J6jDEmsqwK/MYYYyKzwG+MMTkmawO/WsWPMcYElbWB3xhjTHAW+I0xJsdkVeBXZ7CehvVtPndjjAklqwK/n4hYbqcxxoTgKfCLyCARKRGRUhEZEeT1u0RkkYjMF5EPReRw12sVIjLX+TchcFtjjDGpFXEiFhHJA8YA5wJrgFkiMkFVF7lWmwMUqepuEbkFeAi43Hltj6r2SGyxjTHGxMrLFX9voFRVl6vqfnyTqQ91r6CqU1V1t/N0Br5J1dOqTh1JdxGMMSYjeQn87YHVrudrnGWhXAe863reQESKRWSGiFwYbAMRudFZp7isrMxDkSK794JjErIfY4zJNglt3BWRHwNFwMOuxYerahFwBfCYiHQN3E5Vx6pqkaoWFRQUxHz8BvV82Tw39e9C84b1Gd67U8z7MsaYbOVlsvW1QEfX8w7OsmpEZAAwEjhDVff5l6vqWuf/5SLyMdATWBZHmUOql1eHlQ8OrnputT3GGFOTlyv+WUA3EeksIvWBYUC17BwR6Qk8AwxR1Y2u5c1EJN953BI4DXA3ChtjjEmxiIFfVcuB24DJwGJgvKouFJHRIjLEWe1hoBHwekDa5jFAsYjMA6YCDwZkAyVVj45NU3UoY4ypNcTf2zVTFBUVaXFxcUL2paqs3ryH/g9PTcj+jDEm2dzV1dEQkdlOe2pEWdlz109E6NTi0HQXwxhjMkpWB/5oPXzpCRzTtkm6i2GMMUmVE4F/3I2nMHrosRHXO/WIlmRa1ZcxxiRaTgT+U7q04Cd9C8Ous/LBwbRvekhqCmSMMWmUE4E/Gq2aNEh3EYwxJqks8Afo26VFuotgjDFJZYE/QOMGvs7M/bq15NeDjkpzaYwxJvG8DNmQU4b37kSlKsN7d6JeXh0eeq8k3UUyxpiEssAfIK+ORGwINsaY2iwnq3oOt05dxpgclpOB/5NfnVXt+b0XHJ2mkhhjTOrlZOAP1KVlo4jr/GLAkVHvd8kfzq96fHlRRxrUs7fbGJN+ORuJ/u/c6AL5IfWjf6vq1z24zZ8vPYE/XnR81PswxphEy6nAP/6mvrx7Rz8Afn5ONwYc0wqAVA3ScPFJaZ+KOKhfDoz+bsYYU3vlVFZP787NA5b4puhK5Pg89evWYX95ZVTb9O7cnCv7dOLI1o05/2+fJawsXp1indaMySk5dcWfKK9c3yfka6N+0D3q/Y2/qS9De7S3kUGNMSnhKfCLyCARKRGRUhEZEeT1u0RkkYjMF5EPReRw12tXi8hS59/ViSx8upx2RMuQr7VvFt9Abyd2OCyu7f1O6RJ4d1PTFX06MfaqXnRoZumtpro+Ne6OTTaJGPhFJA8YA5wPdAeGi0jgZe0coEhVTwDeAB5ytm0O3Af0AXoD94lIs8QVP/vUywv+kQzs3jritq/eEPpOJJg/XnQ8A49tQ5vDbGA6U12zQ+unuwgmibxc8fcGSlV1uaruB8YBQ90rqOpUVd3tPJ0B+FsxzwM+UNXNqroF+AAYlJiiJ064Gv6WjWL7Abi3O6lT05j24XbfkOrzCTz945PCrt+vWwEAY64Iv55foXVqMyZneAn87YHVrudrnGWhXAe8G822InKjiBSLSHFZWZmHIiWGSOR1Jt7ej5d+2jvsOs/95OA0lwKU/GEQn484B4AP/+8MXrouuitxgBYNa55wWjXOB+CfP+3NUW1qtge4r9IKWzRk5YODGXxC22r9Cfz8GU1+dep4eDOCOOPIgpi2i0bvwvRWO9hJ0WSbhDbuisiPgSLg4Wi2U9WxqlqkqkUFBckPJNFo3aQB/V3B7YZ+nWus06WgYbXn+XXzqnL4uxY0olF+9MlTPTvVrBF79PIenNSpKad1bUHnlg1rvN6pefAA5e5P4Cdhznp5UZwE/nLZiZ7XrZlV5c34m/vGtF2ijL8pvcdPBy8XRaa6o9s0TncRPPMS+NcCHV3POzjLqhGRAcBIYIiq7otm29rOHdi7FkTuBex2bDtvmTyCr1H5rZ+dRt0Q7QDRCPe7Pq699wbmAucuJJSmh9aretwmhkluRpyf3uE0OjU/1CbnAa465fDIK+W4m8/omu4ieOYlgswCuolIZxGpDwwDJrhXEJGewDP4gv5G10uTgYEi0sxp1B3oLMsIt5/djVaN8+PKYJhw22nVAkPHEFfd0Ur5FVeGTjXcpEG9yCsBfx/ekwHH1GwAb9ko/IkplIt7tqdV43x+G0N6bja6/8Lj0l2EjBd455/JIgZ+VS0HbsMXsBcD41V1oYiMFpEhzmoPA42A10VkrohMcLbdDNyP7+QxCxjtLMsIx3c4jC9HDqBpFBkMgX29TujQNLGFisGL15xM787NaVAvr2pZw/y8MFtE55IYehyPvaoX/bq15OWf9qF1k9iCL8DlJ/tuGN29i1c+OLjGej88sR1tE5Sd1KBeHR669AS+HDmAcz1kU2WjXodb8l0281RnoKqTVPVIVe2qqg84y0apqj/AD1DV1qraw/k3xLXtC6p6hPPvxeT8GcknYStHEi/wBBPuDuCso1sx/qa+uKvm4210nXffwKrHf/3RwXr81z3Wtw88tg0vX9eH4zscxs/P7hZTGS4v6ljV3nDW0a0irB2kDN1bV82oFo0WDfMTUp1Wm113es22rFzw+s19+elp0f/tv/1B95THiHjk9rc7A1wb8CXzB/jzj2sT137DNd66jxPKYYdUr2K5qX8X+nVryclhMmzc9flu/nNYtNVX7s5wx7aLvmPb2J8UVTtmYONyx+aHMPaqXlHvN1aBdyRFCbiq/knf5NS9i0jIzzObxXrXOOzkjrWqQdwCf5pd2it4NUrH5oey8sHBMTWIxiJSFf89FxzDyxHSUkdecEzY16P9XQTe9cz57bnV7kQC3di/S5RH8N2ZpIr777n/wuOiqmIMpanrBN0qQkN7tJ51pSkHCpYplonOjuFOMRYNY8jcS6fa8elloXaHNaCDh+Edbj3LlylQG3pShup1HOiQenkxXe02a1i/xp2Iv3OcqladLKvREI/T7JB6iWmDKUpiH4dwd3epMrRHO373w5oN7Me1j5wN17tzc/6QwkZpu+I3EX1+zzlMu/vsiOtd1beQlQ8OrtZwmwgd0zA+j7/6SQTeuOVUzolwNeblh/T81SfzwS/6R6zaqk1uPqMrf/XYP+LUri344Bf9gfQHHv+Q536f/OrMuPf5o6KODDqubY3lXurTBWjX9BB+Pegoz8eLJQvMywWc21lHFXBc+yaMv6kvn484O+rtE8ECfxbykoLYrmn1L9ttZx1R9bhlkF7D4bhvp9+7s1/QKzSoWdWTiAvwQ/Pz6NY6to4zyWqMC9WpTT3+xV0KGnJJQBXg0B7tgq4rIjSKoQE7Ef59wylMueuMqueBo8se3iL+9MY6cZzN/Jt67Vh1Ysem1S6wbj8nclLCk1eexCTnhOf1+9Qwvy7/+3k/enduTrumh9S4i00FC/xZRERY+eDgmDIy3IHGncXjhTtz5ug2TbgmRFZEg/p53HxGV8+ZQbG6sEc7/nHtyUnb/+Q7+4d9PVS7jZs7RPw+YBwmP3fgOcpD8Ep2Vsm8UdXbV/p2bcERraLrsOhX8gdvQ3ZF28emZaP6DDu5Y+QVg3hieM9qz5t4OKG2b3qI574mfplQ41i7WiQy2Ju39GXXvop0FyMhEtHo6HZyoa8+/7xj24RMMx1wTGumLN4Q9b6DBbvHhh38ASf6R9anc3NPQTiYaAOz1yG6o51H6NSuLZjz7Vb2HIju+xpL35AOzQ5hzZY9NZZ7vZIPNYZUr8ObUbJhR41Jj57+cS/2V1QybtbqoNuF4+98GetNRqzbJXAeKM/sij9Beh3evNqYPrH65cCjKGicT3ePQzlE6wcn1Kwv9Qu8JR55wTEJmSf46DZNWPng4LB9Cy7sebAqo1uMV5Gp8FoKxu3xxw93FZqnOm2PgefVG05hzqhzoy5X3bw6PHZ5D98Tj8EqsN7f7fMRZ/PenaFfD6dxg7pB2xCaBammjPdO6MjW3r6PWdVz16RWny4tmDVyQEwDu3kRrvHqjVtOZdrdZ1U9v6F/F67o0ykp5YDQg8oNirMPQ0jO779xClLvPv7lmZ7XDTX1p9cGa38dfzSpi7EmC5x/fHSfTWNXNcgL1xxMD80ToV3TQzg6yCizfl5HRW3TpAFd3IMWJvAK+n8/Pz3sIH3NXSea/Lrh39NMGuXVAr+p0ii/bkyzcfV3xv4/MspG1nsuOJr8OPPBoxlN2r/qyMHB+xvUd9JRox2m4ZYzuzJ9RPUMLS953YnKwmnSoB4z7z2H3w85tkanq4t7Vh8FPZoTUqL5hzdp3rB+xGHAJ9x2Gm//7LSw6/hTYkMNKpiINo/j2h8WtuozmrG5Tg0zc1+qWeCvxRb8/jwW/P68dBeDS3p1YN59A6Ounsqvm1fV6aogxsHUIg2tEOxqOlTA7djcl+l0pYe7HHejpuBr5Kt23CCXne5l0Qb9ennhN2jdpAF18+pwZKuDJ9+fn30Ef7ioeh57OlIH/aL5k0/o0LSq2qZlo/o1xg4SfG1R79x6Go8P75GwMkbDfQfjRai6/DFXHpws6dD6iU3bDsUCv0dNDvFdwfmvqO4690hO7doinUWiUX7dpFUJheNO4fOLNSXtjnO68er1fejT5eB76e9jEBhM49XCmRUt0i15sKDcvGH9akMTT7nrDH51nvf88Hj9yJWp8q8wPagDTziH1q8b8HrtUzevDm/ecmrQ13p0bFrjb0yVs49OzAB+nVs2rJpa9YQEzbkdiWX1eHRpr47sr1AuL/L9AG8/p5unPN9Ajw/vyYqyXYkunmc/O7MrK77fVSNPPBpHtGrE/RceR8+OTeMuT928OjVugS8r6kDbpg04PcG3xn+6+AROO6IlZx3lqwv3/9j+eNHxfL7sey7q2Z5fvTE/aHXXV7+NvjHUq8C6/GB1+/Xz6tCtVSOWbtxJqxhHO33okhM89a6O9ko2E9TGE5rbNacV8v6i6LPaYmWB36O8OpKQySiGnBi8I06qtGrSgH9GmErSi2ROzCEiVXMGJ9Jhh9Tjyj6+chf/ZkDVMBhX9OlU1YidqADfomHN4JyOtD23HwXktz986Qn86o35/PS0zrwwfQXgm9YzXPaVv948ngHcQjVmJ4JwcHKgExNwYRKoVeN8Nu7YF3lFl75dWvDF8k0JL0s8LPDniH/fcArLv9+Z7mJkjFgnaHELF8Dy6gjHtmvCwu+2x3WM/Lp12BeQq+5Fs0PrRRxe+LKijvTs1IzCFocy4JhWHNO2SdB0SLf6devwx4uOj+pu7I8XHc/ysp0hs5RmjRzAJ0vK6N62CRc8/lnI/TQ7tB479pZTXhn+xNGtdWMm3d6vKg2zSYzVkME+3sYN6kYd+K/o04kvlm/ylKacqosDT3X8IjJIREpEpFRERgR5vb+IfCUi5SJyacBrFc7kLFUTtJjU69u1RdXVbi4ZOTj5M2hFaqg9ysl28lezHFIvj3PirB9+/ea+PP3jk0LOj/zMVUVVQbxV43wa1Av+Uz+iVaOq6rZIQd/vij6d6BQiNfF/Pz896Pq/CTOMSEHjfC7t1SFicsCcUQM9z+3QvV2Tqob/Xoc34ylXA2ok4T7OUCnI4fzwxHasfHBw2GlKUz2Wf8TALyJ5wBjgfKA7MFxEAj/Fb4FrgFeD7GJPsAlajEmFdM6g5b96e/CS45k1ckBVZs7E20/nsBiqSgY7ne9aNsrn5MLmQQcvK3TGx3EPo/HFPeew4Hepyf6KZr7mVDr/+OAdF685tTCq/fzIaeMbWMtnZvNS1dMbKFXV5QAiMg4YCizyr6CqK53Xor8nNcbx/i/6s2NvebqLkXD18upUu9rzV3lce1ohHyzaQE9naOmq10Ps5/azu3Hd6Z2rdYoKNHrocZx/fJtqA6b57gqSd0XZ6/BmLF53sErr01+dlbTx+s88qoBHpyyJaUa2YLq3jS4FubPTOzeujL4MaIn2EvjbA+6BL9YA4WfkqK6BiBQD5cCDqvpO4AoiciNwI0CnTsnrKWoyW7QdwDLNM1f14q2v1nhe/9SuLVn54GDWbas5lk0wdepI2KAPcEj9vISlGXoVmGoZqhooEU7s2DTonMvROLd7az5YtIEb+nXmsqIOdG3VkGlLvTW+Ht2mCTPuOcfzPNLuKrZwVYL+HsCp+g2konH3cFVdKyJdgI9E5GtVXeZeQVXHAmMBioqKMuB8aGqDaEaGTMU4Kucd24bzwszodc4xrXl+2ooafR7C9Ss4um0T5q3emvax9jNdq8b5rPh+l6d0Vf/73611Y0SEXoc3p9fh3kcBbRPF9Iwz7x3gab2j2jTmtRtPoUfA3V+yeAn8awF3HlgHZ5knqrrW+X+5iHwM9ASWhd3ImAhKHzjf81g2U+46I2zDWqz8+/Ta6/jeC47hpv5dqo3vAr6rvV8MOJJHpyypsc0/rz2ZReu2R+x0Vhv4R/e8vl/0U2RG8uSVJ/FxSVnYIRQm39mf2au28NW3WxJ+/FCi6djo7sSYbF4C/yygm4h0xhfwhwFXeNm5iDQDdqvqPhFpCZwGPBRrYY3xizRUg1usY8ZHclmvjjTMr8sFQRpZg8mrI7QKMYfyqUe04NEpNZc3PbQ+p3bNnDFe4pFfNy/uappQWjTKj9gp8ag2jTmqTeOUBv5MFfHXo6rlwG3AZGAxMF5VF4rIaBEZAiAiJ4vIGuAy4BkRWehsfgxQLCLzgKn46vgX1TyKMbVPnTrCD05oF3HAsWhZtY5JNk91/Ko6CZgUsGyU6/EsfFVAgdt9DsQ/oLsxtdCJHQ9j0brtaZlaz8TupMObwbQVSZsTIxNYz11jkuR3Q45leO9OUQ3da9LvguPbMuOec2o04p6ToBRSr3MvJ5MFfmOSJL9uXtUY9MabsVf1onPL5GZg3T3oaCoqNey4WYFBf9rdZ8U9zEeqe+eGY4HfGJMxBoZJh02Ugsb5POqfQtKjWCYoymQ2Hr8xGcYad2uvey84mvE39aVPZ+/9AtLBrviNMSH9bVgPXpn5bbqLUWvc2L8rAC9eezIbtkc3imcqWeA3xoQ0tEd7hvZoH3lFU82h9evSuWXmhler6jEmA7hnM8urYz/LbNSuqa/B+Nh26R/BNHNPScbkkLp5dZg3aiBPfbKMC45LfgOnSb2enZox8fbTOaZN+vsHWOA3JkMcdmg9Rpx/dLqLYZIoE672wap6jDEm51jgN8aYHGOB3xhjcowFfmOMyTEW+I0xJsdY4DfGmBxjgd8YY3KMBX5jjMkxopr+SQHcRKQMWBXHLloC3yeoOIlk5YqOlSs6Vq7oZGO5DlfVAi8rZlzgj5eIFKtqUbrLEcjKFR0rV3SsXNHJ9XJZVY8xxuQYC/zGGJNjsjHwj013AUKwckXHyhUdK1d0crpcWVfHb4wxJrxsvOI3xhgThgV+Y4zJNaqaFf+AQUAJUAqMSNIxOgJTgUXAQuAOZ/nvgLXAXOffBa5t7nHKVAKcF6m8QGdgprP8NaC+x7KtBL52jl/sLGsOfAAsdf5v5iwX4HHnGPOBk1z7udpZfylwtWt5L2f/pc624qFMR7nek7nAduDOdLxfwAvARmCBa1nS359Qx4hQroeBb5xjvw00dZYXAntc79vTsR4/3N8YplxJ/9yAfOd5qfN6oYdyveYq00pgbhrer1CxIe3fsaC/h2QEyFT/A/KAZUAXoD4wD+iehOO09X9AQGNgCdDd+UH8Msj63Z2y5Dtf9GVOWUOWFxgPDHMePw3c4rFsK4GWAcsewvmxASOAPzuPLwDedb58pwAzXV+g5c7/zZzH/i/ql8664mx7fgyf0Xrg8HS8X0B/4CSqB4ykvz+hjhGhXAOBus7jP7vKVeheL2A/UR0/1N8YoVxJ/9yAn+EEaGAY8FqkcgW8/ldgVBrer1CxIe3fsaB/f7TBLxP/AX2Bya7n9wD3pOC4/wHODfODqFYOYLJT1qDldT7Q7zn4o6+2XoSyrKRm4C8B2rq+mCXO42eA4YHrAcOBZ1zLn3GWtQW+cS2vtp7H8g0EpjuP0/J+ERAIUvH+hDpGuHIFvHYR8Eq49WI5fqi/McL7lfTPzb+t87ius56EK5druQCrgW7peL8CjuGPDRnxHQv8ly11/O3xfeB+a5xlSSMihUBPfLejALeJyHwReUFEmkUoV6jlLYCtqloesNwLBd4XkdkicqOzrLWqrnMerwdax1iu9s7jwOXRGAb82/U83e8XpOb9CXUMr36K7+rOr7OIzBGRT0Skn6u80R4/1t9Msj+3qm2c17c563vRD9igqktdy1L+fgXEhoz8jmVL4E8pEWkEvAncqarbgaeArkAPYB2+281UO11VTwLOB24Vkf7uF9V3OaBpKBciUh8YArzuLMqE96uaVLw/0R5DREYC5cArzqJ1QCdV7QncBbwqIk2SdfwgMu5zCzCc6hcXKX+/gsSGuPYXLa/HyJbAvxZf44pfB2dZwolIPXwf7Cuq+haAqm5Q1QpVrQSeBXpHKFeo5ZuApiJSN9q/Q1XXOv9vxNcg2BvYICJtnXK3xdcoFku51jqPA5d7dT7wlapucMqY9vfLkYr3J9QxwhKRa4AfAFc6P2ZUdZ+qbnIez8ZXf35kjMeP+jeTos+tahvn9cOc9cNy1r0YX0Ovv7wpfb+CxYYY9peS71i2BP5ZQDcR6excXQ4DJiT6ICIiwPPAYlV9xLW8rWu1i4AFzuMJwDARyReRzkA3fA00Qcvr/MCnApc621+Nr64wUrkaikhj/2N89ekLnONfHWRfE4CfiM8pwDbnVnEyMFBEmjm38QPx1b2uA7aLyCnOe/ATL+VyqXYllu73yyUV70+oY4QkIoOAXwNDVHW3a3mBiOQ5j7s478/yGI8f6m8MV65UfG7u8l4KfOQ/8UUwAF8deFV1SCrfr1CxIYb9peQ7ltDGznT+w9dKvgTfWX1kko5xOr7bqPm4UtqAl/GlWc13PoS2rm1GOmUqwZUJE6q8+DIgvsSXsvU6kO+hXF3wZUzMw5dKNtJZ3gL4EF+a1xSgubNcgDHOsb8Gilz7+qlz7FLgWtfyInw/9GXAE3hI53S2a4jviu0w17KUv1/4TjzrgAP46kevS8X7E+oYEcpViq+e1/8d82e5XOJ8vnOBr4Afxnr8cH9jmHIl/XMDGjjPS53Xu0Qql7P8H8DNAeum8v0KFRvS/h0L9s+GbDDGmByTLVU9xhhjPLLAb4wxOcYCvzHG5BgL/MYYk2Ms8BtjTI6xwG+MMTnGAr8xxuSY/wfEJ3S4wz9fmgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure the mean, std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.0668270587921143\n",
      "val loss: 2.1049270629882812\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        \"train\": (Xtr, Ytr),\n",
    "        \"val\": (Xdev, Ydev),\n",
    "        \"test\": (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]                          # (N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 + b1    # (N, n_hidden)\n",
    "    # hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / (hpreact.std(0, keepdim=True)) + bnbias\n",
    "    hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact)             # (N, n_hidden)\n",
    "    logits = h @ W2 + b2                # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f\"{split} loss: {loss.item()}\")\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlah.\n",
      "amille.\n",
      "khirmyli.\n",
      "taty.\n",
      "sacarsaeja.\n",
      "hutna.\n",
      "merynch.\n",
      "aqui.\n",
      "nellara.\n",
      "chaiir.\n",
      "kaleigh.\n",
      "ham.\n",
      "jore.\n",
      "quinn.\n",
      "saline.\n",
      "liveni.\n",
      "wazell.\n",
      "dearyn.\n",
      "kai.\n",
      "eveinsleen.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        emb = C[torch.tensor([context])] # (1, block_size, n_embd)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special \".\" token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(itos[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46497\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "  \n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "\n",
    "class BatchNorm1d:\n",
    "  \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True) # batch mean\n",
    "            xvar = x.var(0, keepdim=True) # batch variance\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        # update the buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "    \n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "n_embd = 10 # dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of nuerons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden), Tanh(),\n",
    "    Linear(           n_hidden, n_hidden), Tanh(),\n",
    "    Linear(           n_hidden, n_hidden), Tanh(),\n",
    "    Linear(           n_hidden, n_hidden), Tanh(),\n",
    "    Linear(           n_hidden, n_hidden), Tanh(),\n",
    "    Linear(           n_hidden, vocab_size),\n",
    "    ]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # last layer: make less confident\n",
    "    layers[-1].weight *= 0.01\n",
    "    # all other layers: apply gain\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "can't retain_grad on Tensor that has requires_grad=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Batur\\Desktop\\python\\makemore\\makemore_pt3.ipynb Cell 13\u001b[0m line \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Batur/Desktop/python/makemore/makemore_pt3.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# backward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Batur/Desktop/python/makemore/makemore_pt3.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m layers:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Batur/Desktop/python/makemore/makemore_pt3.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     layer\u001b[39m.\u001b[39;49mout\u001b[39m.\u001b[39;49mretain_grad() \u001b[39m# AFTER_DEBUG: would take out retain_graph\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Batur/Desktop/python/makemore/makemore_pt3.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m parameters:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Batur/Desktop/python/makemore/makemore_pt3.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     p\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: can't retain_grad on Tensor that has requires_grad=False"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "    # minibatch construction\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    break # AFTER DEBUG: would take out obviously to run full optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "    if isinstance(layer, Linear):\n",
    "        t = layer.out\n",
    "        print(\"layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%\" % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f\"layer {i} ({layer.__class__.__name__})\")\n",
    "plt.legend(legends)\n",
    "plt.title(\"activation distribution\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
